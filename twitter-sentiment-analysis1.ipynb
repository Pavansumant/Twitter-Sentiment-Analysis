{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gvpavansumanth/twitter-sentiment-analysis1?scriptVersionId=116396079\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## Dataset Information\n\nThe objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n\nFormally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.\n\nFor training the models, we provide a labelled dataset of 31,962 tweets. The dataset is provided in the form of a csv file with each line storing a tweet id, its label and the tweet.","metadata":{}},{"cell_type":"markdown","source":"## Import modules","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nimport nltk\nimport warnings\n%matplotlib inline\n\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:35:27.16703Z","iopub.execute_input":"2023-01-08T07:35:27.167392Z","iopub.status.idle":"2023-01-08T07:35:27.174298Z","shell.execute_reply.started":"2023-01-08T07:35:27.167367Z","shell.execute_reply":"2023-01-08T07:35:27.173377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/twitter/Twitter Sentiments.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:35:44.388624Z","iopub.execute_input":"2023-01-08T07:35:44.389106Z","iopub.status.idle":"2023-01-08T07:35:44.518708Z","shell.execute_reply.started":"2023-01-08T07:35:44.389065Z","shell.execute_reply":"2023-01-08T07:35:44.517733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# datatype info\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:35:52.848536Z","iopub.execute_input":"2023-01-08T07:35:52.849Z","iopub.status.idle":"2023-01-08T07:35:52.880824Z","shell.execute_reply.started":"2023-01-08T07:35:52.848966Z","shell.execute_reply":"2023-01-08T07:35:52.879284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing the dataset","metadata":{}},{"cell_type":"code","source":"# removes pattern in the input text\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for word in r:\n        input_txt = re.sub(word, \"\", input_txt)\n    return input_txt","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:36:37.929172Z","iopub.execute_input":"2023-01-08T07:36:37.929572Z","iopub.status.idle":"2023-01-08T07:36:37.935225Z","shell.execute_reply.started":"2023-01-08T07:36:37.929532Z","shell.execute_reply":"2023-01-08T07:36:37.933919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:36:41.506979Z","iopub.execute_input":"2023-01-08T07:36:41.507348Z","iopub.status.idle":"2023-01-08T07:36:41.520421Z","shell.execute_reply.started":"2023-01-08T07:36:41.507318Z","shell.execute_reply":"2023-01-08T07:36:41.518589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove twitter handles (@user)\ndf['clean_tweet'] = np.vectorize(remove_pattern)(df['tweet'], \"@[\\w]*\")","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:36:46.447154Z","iopub.execute_input":"2023-01-08T07:36:46.448517Z","iopub.status.idle":"2023-01-08T07:36:46.572501Z","shell.execute_reply.started":"2023-01-08T07:36:46.448447Z","shell.execute_reply":"2023-01-08T07:36:46.571406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:36:55.96216Z","iopub.execute_input":"2023-01-08T07:36:55.96259Z","iopub.status.idle":"2023-01-08T07:36:55.975362Z","shell.execute_reply.started":"2023-01-08T07:36:55.962555Z","shell.execute_reply":"2023-01-08T07:36:55.973606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove special characters, numbers and punctuations\ndf['clean_tweet'] = df['clean_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:37:07.677797Z","iopub.execute_input":"2023-01-08T07:37:07.67824Z","iopub.status.idle":"2023-01-08T07:37:07.810306Z","shell.execute_reply.started":"2023-01-08T07:37:07.678206Z","shell.execute_reply":"2023-01-08T07:37:07.808896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove short words\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x: \" \".join([w for w in x.split() if len(w)>3]))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:37:23.007703Z","iopub.execute_input":"2023-01-08T07:37:23.008148Z","iopub.status.idle":"2023-01-08T07:37:23.117389Z","shell.execute_reply.started":"2023-01-08T07:37:23.008106Z","shell.execute_reply":"2023-01-08T07:37:23.11623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# individual words considered as tokens\ntokenized_tweet = df['clean_tweet'].apply(lambda x: x.split())\ntokenized_tweet.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:37:35.867657Z","iopub.execute_input":"2023-01-08T07:37:35.868072Z","iopub.status.idle":"2023-01-08T07:37:35.91931Z","shell.execute_reply.started":"2023-01-08T07:37:35.868039Z","shell.execute_reply":"2023-01-08T07:37:35.917248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stem the words\nfrom nltk.stem.porter import PorterStemmer\nstemmer = PorterStemmer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda sentence: [stemmer.stem(word) for word in sentence])\ntokenized_tweet.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:37:39.536981Z","iopub.execute_input":"2023-01-08T07:37:39.537323Z","iopub.status.idle":"2023-01-08T07:37:45.762526Z","shell.execute_reply.started":"2023-01-08T07:37:39.537297Z","shell.execute_reply":"2023-01-08T07:37:45.761313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# combine words into single sentence\nfor i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = \" \".join(tokenized_tweet[i])\n    \ndf['clean_tweet'] = tokenized_tweet\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:37:49.047456Z","iopub.execute_input":"2023-01-08T07:37:49.048186Z","iopub.status.idle":"2023-01-08T07:37:49.35152Z","shell.execute_reply.started":"2023-01-08T07:37:49.048155Z","shell.execute_reply":"2023-01-08T07:37:49.349909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"pip install wordcloud","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:37:58.717259Z","iopub.execute_input":"2023-01-08T07:37:58.717612Z","iopub.status.idle":"2023-01-08T07:38:09.53534Z","shell.execute_reply.started":"2023-01-08T07:37:58.717583Z","shell.execute_reply":"2023-01-08T07:38:09.534273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# visualize the frequent words\nall_words = \" \".join([sentence for sentence in df['clean_tweet']])\n\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n\n# plot the graph\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:38:15.618274Z","iopub.execute_input":"2023-01-08T07:38:15.618666Z","iopub.status.idle":"2023-01-08T07:38:18.171554Z","shell.execute_reply.started":"2023-01-08T07:38:15.618628Z","shell.execute_reply":"2023-01-08T07:38:18.169778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# frequent words visualization for +ve\nall_words = \" \".join([sentence for sentence in df['clean_tweet'][df['label']==0]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n\n# plot the graph\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:38:29.247998Z","iopub.execute_input":"2023-01-08T07:38:29.248354Z","iopub.status.idle":"2023-01-08T07:38:31.639226Z","shell.execute_reply.started":"2023-01-08T07:38:29.248328Z","shell.execute_reply":"2023-01-08T07:38:31.637692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# frequent words visualization for -ve\nall_words = \" \".join([sentence for sentence in df['clean_tweet'][df['label']==1]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n\n# plot the graph\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract the hashtag\ndef hashtag_extract(tweets):\n    hashtags = []\n    # loop words in the tweet\n    for tweet in tweets:\n        ht = re.findall(r\"#(\\w+)\", tweet)\n        hashtags.append(ht)\n    return hashtags    ","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:39:20.392174Z","iopub.execute_input":"2023-01-08T07:39:20.393208Z","iopub.status.idle":"2023-01-08T07:39:20.400085Z","shell.execute_reply.started":"2023-01-08T07:39:20.393153Z","shell.execute_reply":"2023-01-08T07:39:20.398645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# extract hashtags from non-racist/sexist tweets\nht_positive = hashtag_extract(df['clean_tweet'][df['label']==0])\n\n# extract hashtags from racist/sexist tweets\nht_negative = hashtag_extract(df['clean_tweet'][df['label']==1])","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:39:23.747159Z","iopub.execute_input":"2023-01-08T07:39:23.747514Z","iopub.status.idle":"2023-01-08T07:39:23.817162Z","shell.execute_reply.started":"2023-01-08T07:39:23.747484Z","shell.execute_reply":"2023-01-08T07:39:23.81595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ht_positive[:5]","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:39:26.907638Z","iopub.execute_input":"2023-01-08T07:39:26.908243Z","iopub.status.idle":"2023-01-08T07:39:26.914322Z","shell.execute_reply.started":"2023-01-08T07:39:26.9082Z","shell.execute_reply":"2023-01-08T07:39:26.913305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# unnest list\nht_positive = sum(ht_positive, [])\nht_negative = sum(ht_negative, [])","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:39:34.867813Z","iopub.execute_input":"2023-01-08T07:39:34.868211Z","iopub.status.idle":"2023-01-08T07:39:41.90349Z","shell.execute_reply.started":"2023-01-08T07:39:34.868178Z","shell.execute_reply":"2023-01-08T07:39:41.902422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ht_positive[:5]","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:39:41.905192Z","iopub.execute_input":"2023-01-08T07:39:41.90637Z","iopub.status.idle":"2023-01-08T07:39:41.917713Z","shell.execute_reply.started":"2023-01-08T07:39:41.906322Z","shell.execute_reply":"2023-01-08T07:39:41.916534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq = nltk.FreqDist(ht_positive)\nd = pd.DataFrame({'Hashtag': list(freq.keys()),\n                 'Count': list(freq.values())})\nd.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:39:53.288408Z","iopub.execute_input":"2023-01-08T07:39:53.288814Z","iopub.status.idle":"2023-01-08T07:39:53.358551Z","shell.execute_reply.started":"2023-01-08T07:39:53.28878Z","shell.execute_reply":"2023-01-08T07:39:53.357407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select top 10 hashtags\nd = d.nlargest(columns='Count', n=10)\nplt.figure(figsize=(15,9))\nsns.barplot(data=d, x='Hashtag', y='Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:39:57.797468Z","iopub.execute_input":"2023-01-08T07:39:57.797851Z","iopub.status.idle":"2023-01-08T07:39:58.021697Z","shell.execute_reply.started":"2023-01-08T07:39:57.797814Z","shell.execute_reply":"2023-01-08T07:39:58.020415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"freq = nltk.FreqDist(ht_negative)\nd = pd.DataFrame({'Hashtag': list(freq.keys()),\n                 'Count': list(freq.values())})\nd.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:40:27.047437Z","iopub.execute_input":"2023-01-08T07:40:27.047839Z","iopub.status.idle":"2023-01-08T07:40:27.066018Z","shell.execute_reply.started":"2023-01-08T07:40:27.047806Z","shell.execute_reply":"2023-01-08T07:40:27.064179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select top 10 hashtags\nd = d.nlargest(columns='Count', n=10)\nplt.figure(figsize=(15,9))\nsns.barplot(data=d, x='Hashtag', y='Count')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:40:31.817758Z","iopub.execute_input":"2023-01-08T07:40:31.818108Z","iopub.status.idle":"2023-01-08T07:40:32.032719Z","shell.execute_reply.started":"2023-01-08T07:40:31.818082Z","shell.execute_reply":"2023-01-08T07:40:32.031356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Input Split","metadata":{}},{"cell_type":"code","source":"# feature extraction\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\nbow = bow_vectorizer.fit_transform(df['clean_tweet'])","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:40:39.123403Z","iopub.execute_input":"2023-01-08T07:40:39.124039Z","iopub.status.idle":"2023-01-08T07:40:39.508078Z","shell.execute_reply.started":"2023-01-08T07:40:39.123971Z","shell.execute_reply":"2023-01-08T07:40:39.506405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bow[0].toarray()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(bow, df['label'], random_state=42, test_size=0.25)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:40:42.237337Z","iopub.execute_input":"2023-01-08T07:40:42.237671Z","iopub.status.idle":"2023-01-08T07:40:42.249992Z","shell.execute_reply.started":"2023-01-08T07:40:42.237643Z","shell.execute_reply":"2023-01-08T07:40:42.248477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, accuracy_score","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:40:50.598777Z","iopub.execute_input":"2023-01-08T07:40:50.599195Z","iopub.status.idle":"2023-01-08T07:40:50.605686Z","shell.execute_reply.started":"2023-01-08T07:40:50.599161Z","shell.execute_reply":"2023-01-08T07:40:50.603952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training\nmodel = LogisticRegression()\nmodel.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:40:54.192823Z","iopub.execute_input":"2023-01-08T07:40:54.193226Z","iopub.status.idle":"2023-01-08T07:40:54.32451Z","shell.execute_reply.started":"2023-01-08T07:40:54.193195Z","shell.execute_reply":"2023-01-08T07:40:54.323566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing\npred = model.predict(x_test)\nf1_score(y_test, pred)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:40:57.146633Z","iopub.execute_input":"2023-01-08T07:40:57.14701Z","iopub.status.idle":"2023-01-08T07:40:57.157352Z","shell.execute_reply.started":"2023-01-08T07:40:57.14698Z","shell.execute_reply":"2023-01-08T07:40:57.156356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test,pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use probability to get output\npred_prob = model.predict_proba(x_test)\npred = pred_prob[:, 1] >= 0.3\npred = pred.astype(np.int)\n\nf1_score(y_test, pred)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:41:04.8578Z","iopub.execute_input":"2023-01-08T07:41:04.858178Z","iopub.status.idle":"2023-01-08T07:41:04.871434Z","shell.execute_reply.started":"2023-01-08T07:41:04.858153Z","shell.execute_reply":"2023-01-08T07:41:04.869961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test,pred)","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:41:08.038227Z","iopub.execute_input":"2023-01-08T07:41:08.038749Z","iopub.status.idle":"2023-01-08T07:41:08.048626Z","shell.execute_reply.started":"2023-01-08T07:41:08.038706Z","shell.execute_reply":"2023-01-08T07:41:08.047376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_prob[0][1] >= 0.3","metadata":{"execution":{"iopub.status.busy":"2023-01-08T07:41:13.737827Z","iopub.execute_input":"2023-01-08T07:41:13.738241Z","iopub.status.idle":"2023-01-08T07:41:13.745304Z","shell.execute_reply.started":"2023-01-08T07:41:13.738206Z","shell.execute_reply":"2023-01-08T07:41:13.744159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}